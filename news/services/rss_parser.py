import hashlib
import logging
import requests
import feedparser
import time
from datetime import datetime, timezone, time as datetime_time
from typing import List, Dict, Optional, Tuple
from urllib.parse import urljoin, urlparse
from dataclasses import dataclass
from django.conf import settings
from django.utils import timezone as django_timezone
from django.db import transaction
from ..models import RSSSource, RawArticle

logger = logging.getLogger(__name__)


@dataclass
class ParsedArticle:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–æ—ó —Å—Ç–∞—Ç—Ç—ñ"""
    title: str
    content: str
    summary: str
    url: str
    author: str
    published_at: datetime
    content_hash: str
    
    def __post_init__(self):
        """–ì–µ–Ω–µ—Ä—É—î–º–æ —Ö–µ—à –ø—ñ—Å–ª—è —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó"""
        if not self.content_hash:
            self.content_hash = self._generate_content_hash()
    
    def _generate_content_hash(self) -> str:
        """–ì–µ–Ω–µ—Ä—É—î–º–æ SHA-256 —Ö–µ—à –¥–ª—è –¥–µ—Ç–µ–∫—Ü—ñ—ó –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤"""
        content_for_hash = f"{self.title}{self.content}{self.url}"
        return hashlib.sha256(content_for_hash.encode('utf-8')).hexdigest()


class RSSParserError(Exception):
    """–ë–∞–∑–æ–≤–∞ –ø–æ–º–∏–ª–∫–∞ RSS –ø–∞—Ä—Å–µ—Ä–∞"""
    pass


class RSSFetchError(RSSParserError):
    """–ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è RSS —Ñ—ñ–¥—É"""
    pass


class RSSParseError(RSSParserError):
    """–ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É RSS –∫–æ–Ω—Ç–µ–Ω—Ç—É"""
    pass


class RSSParser:
    """–ì–æ–ª–æ–≤–Ω–∏–π RSS –ø–∞—Ä—Å–µ—Ä –¥–ª—è –Ω–æ–≤–∏–Ω–Ω–∏—Ö –¥–∂–µ—Ä–µ–ª"""
    
    def __init__(self):
        self.session = requests.Session()
        self.target_date = None  
        self.date_filter_enabled = True  
        self.session.headers.update({
            'User-Agent': (
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
                'AppleWebKit/537.36 (KHTML, like Gecko) '
                'Chrome/91.0.4472.124 Safari/537.36 LAZYSOFT-NewsBot/1.0'
            )
        })
        self.session.timeout = 30
        
        # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –ª–æ–≥—É–≤–∞–Ω–Ω—è
        self.logger = logging.getLogger(__name__)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É
        self.stats = {
            'total_sources': 0,
            'successful_sources': 0,
            'failed_sources': 0,
            'total_articles': 0,
            'new_articles': 0,
            'duplicate_articles': 0,
            'errors': []
        }
    
    def parse_all_sources(self, language: Optional[str] = None, 
                         category: Optional[str] = None,
                         active_only: bool = True) -> Dict:
        """
        –ü–∞—Ä—Å–∏—Ç—å –≤—Å—ñ RSS –¥–∂–µ—Ä–µ–ª–∞ –∑ —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—î—é
        
        Args:
            language: –§—ñ–ª—å—Ç—Ä –ø–æ –º–æ–≤—ñ ('en', 'pl', 'uk')
            category: –§—ñ–ª—å—Ç—Ä –ø–æ –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó
            active_only: –¢—ñ–ª—å–∫–∏ –∞–∫—Ç–∏–≤–Ω—ñ –¥–∂–µ—Ä–µ–ª–∞
            
        Returns:
            Dict –∑ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ—é –ø–∞—Ä—Å–∏–Ω–≥—É
        """
        self.logger.info("üöÄ –ü–æ—á–∞—Ç–æ–∫ –ø–∞—Ä—Å–∏–Ω–≥—É RSS –¥–∂–µ—Ä–µ–ª")
        
        # –§—ñ–ª—å—Ç—Ä—É—î–º–æ –¥–∂–µ—Ä–µ–ª–∞
        sources_queryset = RSSSource.objects.all()
        
        if active_only:
            sources_queryset = sources_queryset.filter(is_active=True)
        if language:
            sources_queryset = sources_queryset.filter(language=language)
        if category:
            sources_queryset = sources_queryset.filter(category=category)
        
        sources = list(sources_queryset)
        self.stats['total_sources'] = len(sources)
        
        self.logger.info(f"üìä –ó–Ω–∞–π–¥–µ–Ω–æ {len(sources)} –¥–∂–µ—Ä–µ–ª –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥—É")
        
        # –ü–∞—Ä—Å–∏–º–æ –∫–æ–∂–Ω–µ –¥–∂–µ—Ä–µ–ª–æ
        for source in sources:
            try:
                result = self.parse_single_source(source)
                self.stats['successful_sources'] += 1
                self.stats['total_articles'] += result['total_articles']
                self.stats['new_articles'] += result['new_articles']
                self.stats['duplicate_articles'] += result['duplicate_articles']
                
                self.logger.info(
                    f"‚úÖ {source.name}: {result['new_articles']} –Ω–æ–≤–∏—Ö, "
                    f"{result['duplicate_articles']} –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤"
                )
                
            except Exception as e:
                self.stats['failed_sources'] += 1
                error_msg = f"‚ùå {source.name}: {str(e)}"
                self.stats['errors'].append(error_msg)
                self.logger.error(error_msg, exc_info=True)
        
        self.logger.info(
            f"üèÅ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {self.stats['successful_sources']}/{self.stats['total_sources']} "
            f"–¥–∂–µ—Ä–µ–ª, {self.stats['new_articles']} –Ω–æ–≤–∏—Ö —Å—Ç–∞—Ç–µ–π"
        )
        
        return self.stats
    
    def set_date_filter(self, target_date):
        """
        –í—Å—Ç–∞–Ω–æ–≤–ª—é—î —Ñ—ñ–ª—å—Ç—Ä –ø–æ –¥–∞—Ç—ñ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥—É —Å—Ç–∞—Ç–µ–π
        
        Args:
            target_date: datetime.date - –¥–∞—Ç–∞ –¥–ª—è —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó —Å—Ç–∞—Ç–µ–π
        """
        self.target_date = target_date
        self.date_filter_enabled = True
        self.logger.info(f"üìÖ –í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ —Ñ—ñ–ª—å—Ç—Ä –ø–æ –¥–∞—Ç—ñ: {target_date}")

    def disable_date_filter(self):
        """–í–∏–º–∏–∫–∞—î —Ñ—ñ–ª—å—Ç—Ä –ø–æ –¥–∞—Ç—ñ - –ø–∞—Ä—Å–∏—Ç—å –≤—Å—ñ —Å—Ç–∞—Ç—Ç—ñ"""
        self.date_filter_enabled = False
        self.target_date = None
        self.logger.info("üö´ –§—ñ–ª—å—Ç—Ä –ø–æ –¥–∞—Ç—ñ –≤–∏–º–∫–Ω–µ–Ω–∏–π")

    def is_article_date_valid(self, published_date):
        """
        –ü–µ—Ä–µ–≤—ñ—Ä—è—î —á–∏ —Å—Ç–∞—Ç—Ç—è –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î —Ñ—ñ–ª—å—Ç—Ä—É –ø–æ –¥–∞—Ç—ñ
        
        Args:
            published_date: datetime - –¥–∞—Ç–∞ –ø—É–±–ª—ñ–∫–∞—Ü—ñ—ó —Å—Ç–∞—Ç—Ç—ñ
            
        Returns:
            bool: True —è–∫—â–æ —Å—Ç–∞—Ç—Ç—è –ø—Ä–æ—Ö–æ–¥–∏—Ç—å —Ñ—ñ–ª—å—Ç—Ä
        """
        # –Ø–∫—â–æ —Ñ—ñ–ª—å—Ç—Ä –≤–∏–º–∫–Ω–µ–Ω–∏–π - –≤—Å—ñ —Å—Ç–∞—Ç—Ç—ñ –ø—Ä–æ—Ö–æ–¥—è—Ç—å
        if not self.date_filter_enabled:
            return True
        
        # –Ø–∫—â–æ –Ω–µ –∑–∞–¥–∞–Ω–∞ —Ü—ñ–ª—å–æ–≤–∞ –¥–∞—Ç–∞ - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –≤—á–æ—Ä–∞—à–Ω—é
        if not self.target_date:
            from datetime import timedelta
            yesterday = django_timezone.now().date() - timedelta(days=1)
            self.target_date = yesterday
        
        if not published_date:
            # –Ø–∫—â–æ –¥–∞—Ç–∞ –Ω–µ–≤—ñ–¥–æ–º–∞ - –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ
            return False
        
        article_date = published_date.date()
        return article_date == self.target_date
    
    def parse_single_source(self, source: RSSSource, enhance_with_fulltext: bool = False) -> Dict:
        """
        –ü–∞—Ä—Å–∏—Ç—å –æ–¥–Ω–µ RSS –¥–∂–µ—Ä–µ–ª–æ –∑ –æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–∏–º –≤–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è–º –ø–æ–≤–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç—É
        
        Args:
            source: RSS –¥–∂–µ—Ä–µ–ª–æ
            enhance_with_fulltext: –ß–∏ –≤–∏—Ç—è–≥—É–≤–∞—Ç–∏ –ø–æ–≤–Ω–∏–π —Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ FiveFilters
        """
        self.logger.info(f"üì° –ü–∞—Ä—Å–∏–Ω–≥ –¥–∂–µ—Ä–µ–ª–∞: {source.name}")
        start_time = time.time()
        
        try:
            # 1. –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ —Ç–∞ –ø–∞—Ä—Å–∏–º–æ RSS
            feed = self._fetch_rss_feed(source.url)
            
            # 2. –ü–∞—Ä—Å–∏–º–æ —Å—Ç–∞—Ç—Ç—ñ –∑ RSS
            parsed_articles = self._parse_feed_content(feed, source)
            
            if not parsed_articles:
                self.logger.warning(f"‚ö†Ô∏è –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π —É {source.name}")
                return self._empty_result()
            
            # 3. –§—ñ–ª—å—Ç—Ä—É—î–º–æ –ø–æ –¥–∞—Ç—ñ —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
            if self.date_filter_enabled and self.target_date:
                filtered_articles = self._filter_articles_by_date(parsed_articles)
                filtered_count = len(parsed_articles) - len(filtered_articles)
                if filtered_count > 0:
                    self.logger.info(f"‚è≠Ô∏è –í—ñ–¥—Ñ—ñ–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {filtered_count} —Å—Ç–∞—Ç–µ–π –∑–∞ –¥–∞—Ç–æ—é")
                parsed_articles = filtered_articles
            
            # 4. –ù–û–í–ò–ô –ö–†–û–ö: –ó–±–∞–≥–∞—á–µ–Ω–Ω—è –ø–æ–≤–Ω–∏–º —Ç–µ–∫—Å—Ç–æ–º
            if enhance_with_fulltext and parsed_articles:
                self.logger.info("üîç –ó–±–∞–≥–∞—á–µ–Ω–Ω—è –ø–æ–≤–Ω–∏–º —Ç–µ–∫—Å—Ç–æ–º —á–µ—Ä–µ–∑ FiveFilters...")
                parsed_articles = self.enhance_articles_with_fulltext(parsed_articles, source)
            
            # 5. –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ –≤ RawArticles —ñ –∑–±–µ—Ä—ñ–≥–∞—î–º–æ
            result = self._save_articles_to_db(parsed_articles, source)
            new_articles = result.get('new_articles', 0)
            duplicate_count = result.get('duplicates', 0)
            
            # 6. –û–Ω–æ–≤–ª—é—î–º–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–∂–µ—Ä–µ–ª–∞
            self._update_source_stats(source)
            
            processing_time = time.time() - start_time
            
            result = {
                'total_articles': len(parsed_articles),
                'new_articles': new_articles,
                'duplicate_articles': duplicate_count,
                'errors': 0,
                'processing_time': processing_time
            }
            
            if self.date_filter_enabled:
                result['filtered_articles'] = filtered_count
            
            self.logger.info(
                f"‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {new_articles} –Ω–æ–≤–∏—Ö, "
                f"{duplicate_count} –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤ –∑–∞ {processing_time:.1f}—Å"
            )
            
            return result
            
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É {source.name}: {str(e)}")
            return {'total_articles': 0, 'new_articles': 0, 'duplicate_articles': 0, 'errors': 1}

    
    def _fetch_rss_feed(self, url: str) -> feedparser.FeedParserDict:
        """
        –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î RSS —Ñ—ñ–¥ –∑ URL
        
        Args:
            url: URL RSS —Ñ—ñ–¥—É
            
        Returns:
            Parsed feed data
        """
        try:
            self.logger.debug(f"üì° –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è RSS: {url}")
            
            response = self.session.get(url)
            response.raise_for_status()
            
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ Content-Type
            content_type = response.headers.get('content-type', '').lower()
            if 'xml' not in content_type and 'rss' not in content_type and 'atom' not in content_type:
                self.logger.warning(f"‚ö†Ô∏è –ü—ñ–¥–æ–∑—Ä—ñ–ª–∏–π Content-Type: {content_type} –¥–ª—è {url}")
            
            # –ü–∞—Ä—Å–∏–º–æ —Ñ—ñ–¥
            feed = feedparser.parse(response.content)
            
            if feed.bozo:
                self.logger.warning(f"‚ö†Ô∏è RSS –º–∞—î –ø–æ–º–∏–ª–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥—É: {url}")
            
            if not feed.entries:
                raise RSSFetchError(f"RSS —Ñ—ñ–¥ –ø—É—Å—Ç–∏–π –∞–±–æ –Ω–µ–≤–∞–ª—ñ–¥–Ω–∏–π: {url}")
            
            self.logger.debug(f"üìÑ –ó–Ω–∞–π–¥–µ–Ω–æ {len(feed.entries)} –∑–∞–ø–∏—Å—ñ–≤ –≤ RSS")
            
            return feed
            
        except requests.exceptions.RequestException as e:
            raise RSSFetchError(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ RSS {url}: {str(e)}")
        except Exception as e:
            raise RSSParseError(f"–ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É RSS {url}: {str(e)}")
    
    def _parse_feed_content(self, feed: feedparser.FeedParserDict, 
                          source: RSSSource) -> List[ParsedArticle]:
        """
        –ü–∞—Ä—Å–∏—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç RSS —Ñ—ñ–¥—É
        
        Args:
            feed: Parsed feed data
            source: RSS –¥–∂–µ—Ä–µ–ª–æ
            
        Returns:
            –°–ø–∏—Å–æ–∫ ParsedArticle –æ–±'—î–∫—Ç—ñ–≤
        """
        articles = []
        
        for entry in feed.entries:
            try:
                article = self._parse_single_entry(entry, source)
                if article:
                    articles.append(article)
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫ —Å—Ç–∞—Ç—Ç—ñ —á–µ—Ä–µ–∑ –ø–æ–º–∏–ª–∫—É: {str(e)}")
                continue
        
        self.logger.debug(f"‚úÖ –£—Å–ø—ñ—à–Ω–æ –ø–∞—Ä—Å–µ–Ω–æ {len(articles)} —Å—Ç–∞—Ç–µ–π")
        return articles
    
    def _parse_single_entry(self, entry, source: RSSSource) -> Optional[ParsedArticle]:
        """
        –ü–∞—Ä—Å–∏—Ç—å –æ–¥–Ω—É —Å—Ç–∞—Ç—Ç—é –∑ RSS entry
        
        Args:
            entry: RSS entry
            source: RSS –¥–∂–µ—Ä–µ–ª–æ
            
        Returns:
            ParsedArticle –∞–±–æ None
        """
        try:
            # –ó–∞–≥–æ–ª–æ–≤–æ–∫ (–æ–±–æ–≤'—è–∑–∫–æ–≤–æ)
            title = self._extract_text(entry, 'title')
            if not title:
                raise ValueError("–í—ñ–¥—Å—É—Ç–Ω—ñ–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç–∞—Ç—Ç—ñ")
            
            # URL (–æ–±–æ–≤'—è–∑–∫–æ–≤–æ)
            url = self._extract_link(entry)
            if not url:
                raise ValueError("–í—ñ–¥—Å—É—Ç–Ω—î –ø–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ —Å—Ç–∞—Ç—Ç—é")
            
            # –ö–æ–Ω—Ç–µ–Ω—Ç (–∑ —Ä—ñ–∑–Ω–∏—Ö –ø–æ–ª—ñ–≤)
            content = self._extract_content(entry)
            
            # –ö–æ—Ä–æ—Ç–∫–∏–π –æ–ø–∏—Å
            summary = self._extract_text(entry, 'summary') or content[:500]
            
            # –ê–≤—Ç–æ—Ä
            author = self._extract_author(entry)
            
            # –î–∞—Ç–∞ –ø—É–±–ª—ñ–∫–∞—Ü—ñ—ó
            published_at = self._extract_date(entry)
            
            # –°—Ç–≤–æ—Ä—é—î–º–æ ParsedArticle
            article = ParsedArticle(
                title=title.strip(),
                content=content.strip() if content else "",
                summary=summary.strip() if summary else "",
                url=url,
                author=author.strip() if author else "",
                published_at=published_at,
                content_hash=""  # –ë—É–¥–µ –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ
            )
            
            return article
            
        except Exception as e:
            self.logger.debug(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É entry: {str(e)}")
            return None
    
    def _extract_text(self, entry, field_name: str) -> str:
        """–í–∏—Ç—è–≥—É—î —Ç–µ–∫—Å—Ç –∑ RSS –ø–æ–ª—è –∑ –æ—á–∏—â–µ–Ω–Ω—è–º HTML"""
        try:
            value = getattr(entry, field_name, '')
            if isinstance(value, dict) and 'value' in value:
                value = value['value']
            elif isinstance(value, list) and value:
                value = value[0]
            
            if value:
                # –û—á–∏—â–µ–Ω–Ω—è HTML —Ç–µ–≥—ñ–≤ (–±–∞–∑–æ–≤–µ)
                import re
                value = re.sub(r'<[^>]+>', '', str(value))
                value = re.sub(r'\s+', ' ', value)
                return value.strip()
            
            return ""
        except Exception:
            return ""
    
    def _extract_link(self, entry) -> str:
        """–í–∏—Ç—è–≥—É—î –ø–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ —Å—Ç–∞—Ç—Ç—é"""
        # –ü—Ä–æ–±—É—î–º–æ —Ä—ñ–∑–Ω—ñ –ø–æ–ª—è –¥–ª—è URL
        url = getattr(entry, 'link', '') or getattr(entry, 'guid', '')
        
        if isinstance(url, dict):
            url = url.get('href', '')
        
        # –í–∞–ª—ñ–¥–∞—Ü—ñ—è URL
        if url and self._is_valid_url(url):
            return url
        
        return ""
    
    def _extract_content(self, entry) -> str:
        """–í–∏—Ç—è–≥—É—î –æ—Å–Ω–æ–≤–Ω–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç–∞—Ç—Ç—ñ"""
        # –ü—Ä–æ–±—É—î–º–æ —Ä—ñ–∑–Ω—ñ –ø–æ–ª—è –¥–ª—è –∫–æ–Ω—Ç–µ–Ω—Ç—É
        content_fields = ['content', 'description', 'summary']
        
        for field in content_fields:
            content = getattr(entry, field, '')
            
            if isinstance(content, list) and content:
                content = content[0]
            
            if isinstance(content, dict):
                content = content.get('value', '')
            
            if content and len(str(content).strip()) > 50:  # –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ –¥–æ–≤–∂–∏–Ω–∞ –∫–æ–Ω—Ç–µ–Ω—Ç—É
                return self._extract_text(entry, field)
        
        return ""
    
    def _extract_author(self, entry) -> str:
        """–í–∏—Ç—è–≥—É—î –∞–≤—Ç–æ—Ä–∞ —Å—Ç–∞—Ç—Ç—ñ"""
        author_fields = ['author', 'dc_creator', 'author_detail']
        
        for field in author_fields:
            author = getattr(entry, field, '')
            if isinstance(author, dict):
                author = author.get('name', '') or author.get('email', '')
            if author:
                return str(author)
        
        return ""
    
    def _extract_date(self, entry) -> datetime:
        """–í–∏—Ç—è–≥—É—î —Ç–∞ –ø–∞—Ä—Å–∏—Ç—å –¥–∞—Ç—É –ø—É–±–ª—ñ–∫–∞—Ü—ñ—ó"""
        date_fields = ['published_parsed', 'updated_parsed']
        
        for field in date_fields:
            date_tuple = getattr(entry, field, None)
            if date_tuple:
                try:
                    return datetime(*date_tuple[:6], tzinfo=timezone.utc)
                except (TypeError, ValueError):
                    continue
        
        # –Ø–∫—â–æ –Ω–µ –∑–Ω–∞–π—à–ª–∏ –¥–∞—Ç—É, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –ø–æ—Ç–æ—á–Ω—É
        return django_timezone.now()
    
    def _is_valid_url(self, url: str) -> bool:
        """–ü–µ—Ä–µ–≤—ñ—Ä—è—î –≤–∞–ª—ñ–¥–Ω—ñ—Å—Ç—å URL"""
        try:
            parsed = urlparse(url)
            return bool(parsed.scheme and parsed.netloc)
        except Exception:
            return False
    
    def _save_articles_to_db(self, articles: List[ParsedArticle], 
                           source: RSSSource) -> Dict:
        """
        –ó–±–µ—Ä—ñ–≥–∞—î —Å—Ç–∞—Ç—Ç—ñ –≤ –±–∞–∑—É –¥–∞–Ω–∏—Ö –∑ –ø–µ—Ä–µ–≤—ñ—Ä–∫–æ—é –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤
        
        Args:
            articles: –°–ø–∏—Å–æ–∫ ParsedArticle
            source: RSS –¥–∂–µ—Ä–µ–ª–æ
            
        Returns:
            Dict –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è
        """
        result = {
            'total_articles': len(articles),
            'new_articles': 0,
            'duplicate_articles': 0,
            'errors': 0
        }
        
        self.logger.debug(f"üíæ –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è {len(articles)} —Å—Ç–∞—Ç–µ–π –¥–ª—è {source.name}")
        
        # –û—Ç—Ä–∏–º—É—î–º–æ —ñ—Å–Ω—É—é—á—ñ —Ö–µ—à—ñ –¥–ª—è —à–≤–∏–¥–∫–æ—ó –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤
        existing_hashes = set(
            RawArticle.objects.filter(source=source)
            .values_list('content_hash', flat=True)
        )
        
        with transaction.atomic():
            for article in articles:
                try:
                    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –¥—É–±–ª—ñ–∫–∞—Ç
                    if article.content_hash in existing_hashes:
                        result['duplicate_articles'] += 1
                        continue
                    
                    # –°—Ç–≤–æ—Ä—é—î–º–æ –Ω–æ–≤—É —Å—Ç–∞—Ç—Ç—é
                    raw_article = RawArticle.objects.create(
                        source=source,
                        title=article.title[:2000],  # –û–±–º–µ–∂–µ–Ω–Ω—è –¥–æ–≤–∂–∏–Ω–∏
                        content=article.content,
                        summary=article.summary,
                        original_url=article.url,
                        author=article.author[:200],  # –û–±–º–µ–∂–µ–Ω–Ω—è –¥–æ–≤–∂–∏–Ω–∏
                        published_at=article.published_at,
                        content_hash=article.content_hash,
                        fetched_at=django_timezone.now(),
                        is_processed=False,
                        is_duplicate=False
                    )
                    
                    result['new_articles'] += 1
                    existing_hashes.add(article.content_hash)
                    
                    self.logger.debug(f"‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–æ: {article.title[:50]}...")
                    
                except Exception as e:
                    result['errors'] += 1
                    self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Å—Ç–∞—Ç—Ç—ñ: {str(e)}")
        
        self.logger.info(
            f"üíæ –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {result['new_articles']} –Ω–æ–≤–∏—Ö, "
            f"{result['duplicate_articles']} –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤, {result['errors']} –ø–æ–º–∏–ª–æ–∫"
        )
        
        return result
    
    def cleanup_old_articles(self, days_old: int = 30) -> int:
        """
        –í–∏–¥–∞–ª—è—î —Å—Ç–∞—Ä—ñ –Ω–µ–æ–±—Ä–æ–±–ª–µ–Ω—ñ —Å—Ç–∞—Ç—Ç—ñ
        
        Args:
            days_old: –í—ñ–∫ —Å—Ç–∞—Ç–µ–π –≤ –¥–Ω—è—Ö
            
        Returns:
            –ö—ñ–ª—å–∫—ñ—Å—Ç—å –≤–∏–¥–∞–ª–µ–Ω–∏—Ö —Å—Ç–∞—Ç–µ–π
        """
        cutoff_date = django_timezone.now() - django_timezone.timedelta(days=days_old)
        
        deleted_count, _ = RawArticle.objects.filter(
            fetched_at__lt=cutoff_date,
            is_processed=False
        ).delete()
        
        self.logger.info(f"üßπ –í–∏–¥–∞–ª–µ–Ω–æ {deleted_count} —Å—Ç–∞—Ä–∏—Ö –Ω–µ–æ–±—Ä–æ–±–ª–µ–Ω–∏—Ö —Å—Ç–∞—Ç–µ–π")
        return deleted_count
    

    

    def _save_articles_to_db(self, articles: List[ParsedArticle], source: RSSSource) -> Dict:
        """
        –û–ù–û–í–õ–ï–ù–ê –≤–µ—Ä—Å—ñ—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º —Ñ—ñ–ª—å—Ç—Ä—É –ø–æ –¥–∞—Ç—ñ
        """
        result = {
            'total_articles': len(articles),
            'new_articles': 0,
            'duplicate_articles': 0,
            'filtered_articles': 0,  # –í—ñ–¥—Ñ—ñ–ª—å—Ç—Ä–æ–≤–∞–Ω–æ –ø–æ –¥–∞—Ç—ñ
            'errors': 0
        }
        
        self.logger.debug(f"üíæ –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è {len(articles)} —Å—Ç–∞—Ç–µ–π –¥–ª—è {source.name}")
        
        # –û—Ç—Ä–∏–º—É—î–º–æ —ñ—Å–Ω—É—é—á—ñ —Ö–µ—à—ñ –¥–ª—è —à–≤–∏–¥–∫–æ—ó –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤
        existing_hashes = set(
            RawArticle.objects.filter(source=source)
            .values_list('content_hash', flat=True)
        )
        
        with transaction.atomic():
            for article in articles:
                try:
                    # –ù–û–í–ò–ô –§–Ü–õ–¨–¢–†: –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –¥–∞—Ç—É
                    if not self.is_article_date_valid(article.published_at):
                        result['filtered_articles'] += 1
                        self.logger.debug(f"‚è≠Ô∏è –í—ñ–¥—Ñ—ñ–ª—å—Ç—Ä–æ–≤–∞–Ω–æ –ø–æ –¥–∞—Ç—ñ: {article.title[:50]}...")
                        continue
                    
                    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –¥—É–±–ª—ñ–∫–∞—Ç
                    if article.content_hash in existing_hashes:
                        result['duplicate_articles'] += 1
                        continue
                    
                    # –°—Ç–≤–æ—Ä—é—î–º–æ –Ω–æ–≤—É —Å—Ç–∞—Ç—Ç—é
                    raw_article = RawArticle.objects.create(
                        source=source,
                        title=article.title[:2000],
                        content=article.content,
                        summary=article.summary,
                        original_url=article.url,
                        author=article.author[:200],
                        published_at=article.published_at,
                        content_hash=article.content_hash,
                        fetched_at=django_timezone.now(),
                        is_processed=False,
                        is_duplicate=False
                    )
                    
                    result['new_articles'] += 1
                    existing_hashes.add(article.content_hash)
                    
                    self.logger.debug(f"‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–æ: {article.title[:50]}...")
                    
                except Exception as e:
                    result['errors'] += 1
                    self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Å—Ç–∞—Ç—Ç—ñ: {str(e)}")
        
        # –õ–æ–≥—É–≤–∞–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
        self.logger.info(
            f"üíæ –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {result['new_articles']} –Ω–æ–≤–∏—Ö, "
            f"{result['duplicate_articles']} –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤"
        )
        
        if result['filtered_articles'] > 0:
            filter_info = f"—Ü—ñ–ª—å–æ–≤–∞ –¥–∞—Ç–∞: {self.target_date}" if self.target_date else "—Ñ—ñ–ª—å—Ç—Ä –≤–∏–º–∫–Ω–µ–Ω–∏–π"
            self.logger.info(
                f"üìÖ –í—ñ–¥—Ñ—ñ–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {result['filtered_articles']} —Å—Ç–∞—Ç–µ–π ({filter_info})"
            )
        
        return result


    def get_parsing_statistics(self) -> Dict:
        """–ü–æ–≤–µ—Ä—Ç–∞—î —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–∞—Ä—Å–µ—Ä–∞"""
        return self.stats.copy()



    def enhance_articles_with_fulltext(self, articles: List[ParsedArticle], source: RSSSource) -> List[ParsedArticle]:
        """–ó–±–∞–≥–∞—á—É—î —Å—Ç–∞—Ç—Ç—ñ –ø–æ–≤–Ω–∏–º —Ç–µ–∫—Å—Ç–æ–º —á–µ—Ä–µ–∑ FiveFilters"""
        from .fulltext_extractor import FullTextExtractor
        
        if not articles:
            return articles
            
        enhanced_articles = []
        extractor = FullTextExtractor()
        
        self.logger.info(f"üîç –í–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è –ø–æ–≤–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç—É –¥–ª—è {len(articles)} —Å—Ç–∞—Ç–µ–π...")
        
        for i, article in enumerate(articles, 1):
            self.logger.debug(f"[{i}/{len(articles)}] {article.title[:50]}...")
            
            try:
                # –í–∏—Ç—è–≥—É—î–º–æ –ø–æ–≤–Ω–∏–π —Ç–µ–∫—Å—Ç
                full_content = extractor.extract_article(article.url)
                
                if full_content and len(full_content) > len(article.content or ""):
                    # –Ø–∫—â–æ –ø–æ–≤–Ω–∏–π —Ç–µ–∫—Å—Ç –∫—Ä–∞—â–∏–π - –∑–∞–º—ñ–Ω—é—î–º–æ
                    original_length = len(article.content or "")
                    article.content = full_content
                    
                    # –û–Ω–æ–≤–ª—é—î–º–æ summary —è–∫—â–æ –≤—ñ–Ω –±—É–≤ –∫–æ—Ä–æ—Ç—à–∏–π
                    if len(full_content) > 500:
                        article.summary = full_content[:500] + "..."
                        
                    improvement = len(full_content) - original_length
                    self.logger.info(f"‚úÖ [{i}] Full-text: +{improvement} —Å–∏–º–≤–æ–ª—ñ–≤ ({len(full_content)} total)")
                else:
                    # –ó–∞–ª–∏—à–∞—î–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π RSS –∫–æ–Ω—Ç–µ–Ω—Ç
                    self.logger.debug(f"‚ö†Ô∏è [{i}] RSS –∫–æ–Ω—Ç–µ–Ω—Ç –∫—Ä–∞—â–∏–π –∞–±–æ full-text –ø–æ—Ä–æ–∂–Ω—ñ–π")
                
                enhanced_articles.append(article)
                
            except Exception as e:
                self.logger.warning(f"‚ùå [{i}] –ü–æ–º–∏–ª–∫–∞ full-text –¥–ª—è {article.url}: {e}")
                # –î–æ–¥–∞—î–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω—É —Å—Ç–∞—Ç—Ç—é –Ω–∞–≤—ñ—Ç—å –ø—Ä–∏ –ø–æ–º–∏–ª—Ü—ñ
                enhanced_articles.append(article)
                continue
        
        success_rate = len([a for a in enhanced_articles if len(a.content or "") > 1000]) / len(articles) * 100
        self.logger.info(f"üìä Full-text —É—Å–ø—ñ—à–Ω—ñ—Å—Ç—å: {success_rate:.1f}% ({len(enhanced_articles)}/{len(articles)} —Å—Ç–∞—Ç–µ–π)")
        
        return enhanced_articles
    
    def _filter_articles_by_date(self, articles: List[ParsedArticle]) -> List[ParsedArticle]:
        """–§—ñ–ª—å—Ç—Ä—É—î —Å—Ç–∞—Ç—Ç—ñ –ø–æ –¥–∞—Ç—ñ –ø—É–±–ª—ñ–∫–∞—Ü—ñ—ó"""
        if not self.target_date:
            return articles
        
        filtered_articles = []
        target_date = self.target_date
        
        for article in articles:
            # –ü–æ—Ä—ñ–≤–Ω—é—î–º–æ —Ç—ñ–ª—å–∫–∏ –¥–∞—Ç—É, –±–µ–∑ —á–∞—Å—É
            article_date = article.published_at.date()
            if article_date == target_date:
                filtered_articles.append(article)
        
        return filtered_articles
    
    def _update_source_stats(self, source: RSSSource):
        """–û–Ω–æ–≤–ª—é—î —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–∂–µ—Ä–µ–ª–∞"""
        try:
            # –ü—Ä–æ—Å—Ç–æ –∑–±–µ—Ä—ñ–≥–∞—î–º–æ –¥–∂–µ—Ä–µ–ª–æ (–±–µ–∑ –Ω–µ—ñ—Å–Ω—É—é—á–æ–≥–æ –ø–æ–ª—è last_parsed)
            source.save()
        except Exception as e:
            self.logger.warning(f"–ü–æ–º–∏–ª–∫–∞ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–∂–µ—Ä–µ–ª–∞ {source.name}: {e}")