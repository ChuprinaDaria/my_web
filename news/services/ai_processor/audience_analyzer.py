import logging
import json
from typing import Dict, List, Tuple, Optional
from datetime import datetime, timedelta
from dataclasses import dataclass
from django.utils import timezone
from django.db.models import Q, Count, Avg
from news.models import RawArticle, ProcessedArticle
from news.services.ai_processor.ai_processor_base import AINewsProcessor

logger = logging.getLogger(__name__)


@dataclass
class RelevanceAnalysis:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª—ñ–∑—É —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—ñ —Å—Ç–∞—Ç—Ç—ñ –¥–ª—è –ú–°–ë –∞—É–¥–∏—Ç–æ—Ä—ñ—ó"""
    article_id: int
    title: str
    relevance_score: int  # 1-10
    category_match: str
    target_audience: str  # 'ukraine', 'poland', 'uk', 'general'
    business_impact: str  # 'high', 'medium', 'low'
    implementation_complexity: str  # 'easy', 'medium', 'hard'
    cost_implications: str  # 'low-cost', 'medium-cost', 'high-cost'
    key_benefits: List[str]
    potential_concerns: List[str]
    confidence_level: float  # 0.0-1.0
    analysis_reasoning: str


class AudienceAnalyzer(AINewsProcessor):
    """
    AI –∞–Ω–∞–ª—ñ–∑–∞—Ç–æ—Ä –¥–ª—è —Å–µ–ª–µ–∫—Ü—ñ—ó —Å—Ç–∞—Ç–µ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏—Ö –¥–ª—è –ú–°–ë –∞—É–¥–∏—Ç–æ—Ä—ñ—ó.
    –í–∏–±–∏—Ä–∞—î —Ç–æ–ø-5 –Ω–∞–π–∫—Ä–∞—â–∏—Ö —Å—Ç–∞—Ç–µ–π —â–æ–¥–Ω—è –Ω–∞ –æ—Å–Ω–æ–≤—ñ:
    - –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—ñ –¥–ª—è –º–∞–ª–æ–≥–æ/—Å–µ—Ä–µ–¥–Ω—å–æ–≥–æ –±—ñ–∑–Ω–µ—Å—É
    - –ü—Ä–∞–∫—Ç–∏—á–Ω–æ—Å—Ç—ñ –≤–ø—Ä–æ–≤–∞–¥–∂–µ–Ω–Ω—è  
    - –í–∞—Ä—Ç–æ—Å—Ç—ñ —Ä—ñ—à–µ–Ω—å
    - –†–µ–≥—ñ–æ–Ω–∞–ª—å–Ω–æ—ó —Å–ø–µ—Ü–∏—Ñ—ñ–∫–∏ (–£–∫—Ä–∞—ó–Ω–∞, –ü–æ–ª—å—â–∞, –í–µ–ª–∏–∫–æ–±—Ä–∏—Ç–∞–Ω—ñ—è)
    """
    
    def __init__(self):
        super().__init__()
        
        # –ü—Ä–æ—Ñ—ñ–ª—å —Ü—ñ–ª—å–æ–≤–æ—ó –∞—É–¥–∏—Ç–æ—Ä—ñ—ó LAZYSOFT
        self.audience_profile = {
            "business_size": "small to medium business (1-50 employees)",
            "budget_range": "$100-$5000 monthly for tech solutions",
            "pain_points": [
                "limited budget for technology",
                "manual repetitive processes", 
                "time management challenges",
                "lack of technical expertise",
                "customer service bottlenecks",
                "marketing automation needs",
                "data management issues"
            ],
            "interests": [
                "business automation tools",
                "cost-effective AI solutions", 
                "CRM systems for small business",
                "social media automation",
                "customer service chatbots",
                "marketing automation",
                "productivity tools",
                "e-commerce solutions"
            ],
            "industries": [
                "accounting and bookkeeping",
                "beauty and wellness",
                "e-commerce and retail", 
                "professional services",
                "restaurants and hospitality",
                "real estate",
                "consulting",
                "creative agencies"
            ],
            "geographic_focus": [
                "Ukraine (Ukrainian market specifics)",
                "Poland (Polish business environment)", 
                "United Kingdom (UK regulations and practices)",
                "European Union (GDPR compliance, EU market)"
            ],
            "typical_search_queries": [
                "CRM for small business",
                "automation tools for SMB",
                "chatbot for customer service",
                "social media automation",
                "AI tools for business",
                "cost-effective business solutions"
            ]
        }
        
        # –ö—Ä–∏—Ç–µ—Ä—ñ—ó —Å–∫–æ—Ä–∏–Ω–≥—É (–≤–∞–≥–∞ –∫–æ–∂–Ω–æ–≥–æ —Ñ–∞–∫—Ç–æ—Ä—É)
        self.scoring_weights = {
            "business_relevance": 0.3,      # –ù–∞—Å–∫—ñ–ª—å–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ –¥–ª—è –±—ñ–∑–Ω–µ—Å—É
            "implementation_ease": 0.2,     # –õ–µ–≥–∫—ñ—Å—Ç—å –≤–ø—Ä–æ–≤–∞–¥–∂–µ–Ω–Ω—è  
            "cost_accessibility": 0.2,      # –î–æ—Å—Ç—É–ø–Ω—ñ—Å—Ç—å –∑–∞ –≤–∞—Ä—Ç—ñ—Å—Ç—é
            "practical_impact": 0.15,       # –ü—Ä–∞–∫—Ç–∏—á–Ω–∏–π –≤–ø–ª–∏–≤ –Ω–∞ –±—ñ–∑–Ω–µ—Å
            "regional_relevance": 0.1,      # –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ—Å—Ç—å –¥–ª—è —Ä–µ–≥—ñ–æ–Ω—É
            "trend_importance": 0.05        # –í–∞–∂–ª–∏–≤—ñ—Å—Ç—å —Ç—Ä–µ–Ω–¥—É
        }
        
        logger.info("üéØ AudienceAnalyzer —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–æ –¥–ª—è –ú–°–ë –∞—É–¥–∏—Ç–æ—Ä—ñ—ó")

    def analyze_article_relevance(self, raw_article: RawArticle) -> RelevanceAnalysis:
        """
        –ê–Ω–∞–ª—ñ–∑—É—î —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ—Å—Ç—å —Å—Ç–∞—Ç—Ç—ñ –¥–ª—è –ú–°–ë –∞—É–¥–∏—Ç–æ—Ä—ñ—ó
        
        Args:
            raw_article: –°–∏—Ä–∞ —Å—Ç–∞—Ç—Ç—è –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É
            
        Returns:
            RelevanceAnalysis –∑ –¥–µ—Ç–∞–ª—å–Ω–æ—é –æ—Ü—ñ–Ω–∫–æ—é
        """
        logger.info(f"üîç –ê–Ω–∞–ª—ñ–∑ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—ñ: {raw_article.title[:50]}...")
        
        try:
            # –ü—ñ–¥–≥–æ—Ç–æ–≤–ª—é—î–º–æ –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É
            content_for_analysis = f"""
            Title: {raw_article.title}
            Summary: {raw_article.summary or raw_article.content[:500]}
            Source: {raw_article.source.name}
            Category: {raw_article.source.category}
            """.strip()
            
            # AI –ø—Ä–æ–º–ø—Ç –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—ñ
            analysis_prompt = self._build_analysis_prompt(content_for_analysis)
            
            # –í–∏–∫–ª–∏–∫–∞—î–º–æ AI –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É
            ai_response = self._call_ai_model(analysis_prompt, max_tokens=800)
            
            # –ü–∞—Ä—Å–∏–º–æ –≤—ñ–¥–ø–æ–≤—ñ–¥—å AI
            analysis_data = self._parse_ai_analysis(ai_response)
            
            # –†–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ —Ñ—ñ–Ω–∞–ª—å–Ω–∏–π —Å–∫–æ—Ä
            final_score = self._calculate_final_relevance_score(analysis_data)
            
            # –°—Ç–≤–æ—Ä—é—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª—ñ–∑—É
            analysis = RelevanceAnalysis(
                article_id=raw_article.id,
                title=raw_article.title,
                relevance_score=final_score,
                category_match=analysis_data.get('category_match', 'general'),
                target_audience=analysis_data.get('target_audience', 'general'),
                business_impact=analysis_data.get('business_impact', 'medium'),
                implementation_complexity=analysis_data.get('implementation_complexity', 'medium'),
                cost_implications=analysis_data.get('cost_implications', 'medium-cost'),
                key_benefits=analysis_data.get('key_benefits', []),
                potential_concerns=analysis_data.get('potential_concerns', []),
                confidence_level=analysis_data.get('confidence_level', 0.7),
                analysis_reasoning=analysis_data.get('reasoning', '')
            )
            
            logger.info(f"‚úÖ –ê–Ω–∞–ª—ñ–∑ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: —Å–∫–æ—Ä {final_score}/10, –∫–∞—Ç–µ–≥–æ—Ä—ñ—è {analysis.category_match}")
            return analysis
            
        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∞–Ω–∞–ª—ñ–∑—É —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—ñ: {e}")
            
            # –§–æ–ª–±–µ–∫ –∞–Ω–∞–ª—ñ–∑ –±–µ–∑ AI
            return self._create_fallback_analysis(raw_article)

    def get_daily_top_articles(self, date: Optional[datetime.date] = None, limit: int = 5) -> List[Tuple[RawArticle, RelevanceAnalysis]]:
        """
        –û—Ç—Ä–∏–º—É—î —Ç–æ–ø —Å—Ç–∞—Ç—Ç—ñ –∑–∞ –¥–µ–Ω—å –Ω–∞ –æ—Å–Ω–æ–≤—ñ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—ñ –¥–ª—è –ú–°–ë
        
        Args:
            date: –î–∞—Ç–∞ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É (–∑–∞ –∑–∞–º–æ–≤—á–∞–Ω–Ω—è–º —Å—å–æ–≥–æ–¥–Ω—ñ)
            limit: –ö—ñ–ª—å–∫—ñ—Å—Ç—å —Å—Ç–∞—Ç–µ–π –¥–ª—è –ø–æ–≤–µ—Ä–Ω–µ–Ω–Ω—è
            
        Returns:
            –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂—ñ–≤ (RawArticle, RelevanceAnalysis) –≤—ñ–¥—Å–æ—Ä—Ç–æ–≤–∞–Ω–∏—Ö –∑–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ—Å—Ç—é
        """
        if not date:
            date = timezone.now().date()
        
        logger.info(f"üìä –ü–æ—à—É–∫ —Ç–æ–ø-{limit} —Å—Ç–∞—Ç–µ–π –∑–∞ {date}")
        
        # –û—Ç—Ä–∏–º—É—î–º–æ –Ω–µ–æ–±—Ä–æ–±–ª–µ–Ω—ñ —Å—Ç–∞—Ç—Ç—ñ –∑–∞ –¥–µ–Ω—å
        daily_articles = RawArticle.objects.filter(
            fetched_at__date=date,
            is_processed=False,
            is_duplicate=False
        ).select_related('source').order_by('-published_at')
        
        if not daily_articles.exists():
            logger.warning(f"‚ö†Ô∏è –ù–µ–º–∞—î —Å—Ç–∞—Ç–µ–π –∑–∞ {date}")
            return []
        
        logger.info(f"üìÑ –ó–Ω–∞–π–¥–µ–Ω–æ {daily_articles.count()} —Å—Ç–∞—Ç–µ–π –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É")
        
        # –ê–Ω–∞–ª—ñ–∑—É—î–º–æ –∫–æ–∂–Ω—É —Å—Ç–∞—Ç—Ç—é
        analyzed_articles = []
        
        for article in daily_articles[:20]:  # –û–±–º–µ–∂—É—î–º–æ –¥–æ 20 –¥–ª—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ
            try:
                analysis = self.analyze_article_relevance(article)
                analyzed_articles.append((article, analysis))
                
                logger.debug(f"üìù {article.title[:30]}... ‚Üí —Å–∫–æ—Ä {analysis.relevance_score}")
                
            except Exception as e:
                logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∞–Ω–∞–ª—ñ–∑—É —Å—Ç–∞—Ç—Ç—ñ {article.id}: {e}")
                continue
        
        # –°–æ—Ä—Ç—É—î–º–æ –∑–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ—Å—Ç—é (–Ω–∞–π–≤–∏—â–∏–π —Å–∫–æ—Ä –ø–µ—Ä—à–∏–º)
        analyzed_articles.sort(key=lambda x: x[1].relevance_score, reverse=True)
        
        # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ —Ç–æ–ø —Å—Ç–∞—Ç—Ç—ñ
        top_articles = analyzed_articles[:limit]
        
        logger.info(f"üéØ –í–∏–±—Ä–∞–Ω–æ —Ç–æ–ø-{len(top_articles)} —Å—Ç–∞—Ç–µ–π:")
        for i, (article, analysis) in enumerate(top_articles, 1):
            logger.info(f"   {i}. [{analysis.relevance_score}/10] {article.title[:60]}...")
        
        return top_articles

    def _build_analysis_prompt(self, content: str) -> str:
        """–°—Ç–≤–æ—Ä—é—î AI –ø—Ä–æ–º–ø—Ç –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—ñ"""
        
        prompt = f"""
        Analyze this tech article for relevance to small-medium business (SMB) audience.

        ARTICLE CONTENT:
        {content}

        TARGET AUDIENCE PROFILE:
        - Business size: {self.audience_profile['business_size']}
        - Budget: {self.audience_profile['budget_range']}
        - Pain points: {', '.join(self.audience_profile['pain_points'][:5])}
        - Interests: {', '.join(self.audience_profile['interests'][:5])}
        - Industries: {', '.join(self.audience_profile['industries'][:4])}
        - Regions: Ukraine, Poland, UK, EU

        ANALYSIS CRITERIA:
        1. Business Relevance (1-10): How relevant is this for SMB operations?
        2. Implementation Ease (easy/medium/hard): How difficult to implement?
        3. Cost Accessibility (low-cost/medium-cost/high-cost): Affordable for SMB?
        4. Business Impact (high/medium/low): Potential impact on business results?
        5. Regional Relevance (ukraine/poland/uk/general): Most relevant region?

        OUTPUT FORMAT (JSON only, no additional text):
        {{
            "relevance_score": 7,
            "category_match": "automation",
            "target_audience": "ukraine", 
            "business_impact": "high",
            "implementation_complexity": "easy",
            "cost_implications": "low-cost",
            "key_benefits": ["saves time", "reduces costs", "improves efficiency"],
            "potential_concerns": ["requires training", "initial setup time"],
            "confidence_level": 0.8,
            "reasoning": "This article discusses affordable automation tools perfect for SMB with clear ROI and easy implementation."
        }}

        Respond with ONLY the JSON object, no additional text.
        """
        
        return prompt

    def _parse_ai_analysis(self, ai_response: str) -> Dict:
        """–ü–∞—Ä—Å–∏—Ç—å –≤—ñ–¥–ø–æ–≤—ñ–¥—å AI —É —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω—ñ –¥–∞–Ω—ñ"""
        try:
            # –û—á–∏—â–∞—î–º–æ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –≤—ñ–¥ markdown –±–ª–æ–∫—ñ–≤
            cleaned_response = ai_response.strip()
            if cleaned_response.startswith('```json'):
                cleaned_response = cleaned_response[7:]
            if cleaned_response.startswith('```'):
                cleaned_response = cleaned_response[3:]
            if cleaned_response.endswith('```'):
                cleaned_response = cleaned_response[:-3]
            
            cleaned_response = cleaned_response.strip()
            
            # –ü–∞—Ä—Å–∏–º–æ JSON
            analysis_data = json.loads(cleaned_response)
            
            # –í–∞–ª—ñ–¥—É—î–º–æ —Ç–∞ –Ω–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ –¥–∞–Ω—ñ
            analysis_data['relevance_score'] = max(1, min(10, int(analysis_data.get('relevance_score', 5))))
            analysis_data['confidence_level'] = max(0.0, min(1.0, float(analysis_data.get('confidence_level', 0.7))))
            
            # –ó–∞–±–µ–∑–ø–µ—á—É—î–º–æ —â–æ —Å–ø–∏—Å–∫–∏ —ñ—Å–Ω—É—é—Ç—å
            analysis_data['key_benefits'] = analysis_data.get('key_benefits', [])[:5]  # –ú–∞–∫—Å–∏–º—É–º 5
            analysis_data['potential_concerns'] = analysis_data.get('potential_concerns', [])[:3]  # –ú–∞–∫—Å–∏–º—É–º 3
            
            return analysis_data
            
        except (json.JSONDecodeError, ValueError, KeyError) as e:
            logger.warning(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É AI –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ: {e}")
            logger.debug(f"AI –≤—ñ–¥–ø–æ–≤—ñ–¥—å: {ai_response}")
            
            # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ –¥–µ—Ñ–æ–ª—Ç–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è
            return {
                'relevance_score': 5,
                'category_match': 'general',
                'target_audience': 'general',
                'business_impact': 'medium',
                'implementation_complexity': 'medium',
                'cost_implications': 'medium-cost',
                'key_benefits': ['potential business value'],
                'potential_concerns': ['requires evaluation'],
                'confidence_level': 0.5,
                'reasoning': 'AI analysis failed, using default scoring'
            }

    def _calculate_final_relevance_score(self, analysis_data: Dict) -> int:
        """–†–æ–∑—Ä–∞—Ö–æ–≤—É—î —Ñ—ñ–Ω–∞–ª—å–Ω–∏–π —Å–∫–æ—Ä —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—ñ –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º –≤—Å—ñ—Ö —Ñ–∞–∫—Ç–æ—Ä—ñ–≤"""
        
        base_score = analysis_data.get('relevance_score', 5)
        
        # –ë–æ–Ω—É—Å–∏ —Ç–∞ —à—Ç—Ä–∞—Ñ–∏
        adjustments = 0
        
        # –ë–æ–Ω—É—Å –∑–∞ –ª–µ–≥–∫–µ –≤–ø—Ä–æ–≤–∞–¥–∂–µ–Ω–Ω—è
        if analysis_data.get('implementation_complexity') == 'easy':
            adjustments += 1
        elif analysis_data.get('implementation_complexity') == 'hard':
            adjustments -= 1
        
        # –ë–æ–Ω—É—Å –∑–∞ –Ω–∏–∑—å–∫—É –≤–∞—Ä—Ç—ñ—Å—Ç—å
        if analysis_data.get('cost_implications') == 'low-cost':
            adjustments += 1
        elif analysis_data.get('cost_implications') == 'high-cost':
            adjustments -= 1
        
        # –ë–æ–Ω—É—Å –∑–∞ –≤–∏—Å–æ–∫–∏–π –±—ñ–∑–Ω–µ—Å-–≤–ø–ª–∏–≤
        if analysis_data.get('business_impact') == 'high':
            adjustments += 1
        elif analysis_data.get('business_impact') == 'low':
            adjustments -= 1
        
        # –ë–æ–Ω—É—Å –∑–∞ —Ä–µ–≥—ñ–æ–Ω–∞–ª—å–Ω—É —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ—Å—Ç—å
        if analysis_data.get('target_audience') in ['ukraine', 'poland', 'uk']:
            adjustments += 0.5
        
        # –®—Ç—Ä–∞—Ñ –∑–∞ –Ω–∏–∑—å–∫—É –≤–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—å AI
        confidence = analysis_data.get('confidence_level', 0.7)
        if confidence < 0.6:
            adjustments -= 1
        
        # –û–±—á–∏—Å–ª—é—î–º–æ —Ñ—ñ–Ω–∞–ª—å–Ω–∏–π —Å–∫–æ—Ä
        final_score = base_score + adjustments
        
        # –û–±–º–µ–∂—É—î–º–æ –¥–æ –¥—ñ–∞–ø–∞–∑–æ–Ω—É 1-10
        return max(1, min(10, round(final_score)))

    def _create_fallback_analysis(self, raw_article: RawArticle) -> RelevanceAnalysis:
        """–°—Ç–≤–æ—Ä—é—î —Ñ–æ–ª–±–µ–∫ –∞–Ω–∞–ª—ñ–∑ —è–∫—â–æ AI –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π"""
        
        # –ü—Ä–æ—Å—Ç–∏–π –∞–Ω–∞–ª—ñ–∑ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤
        title_lower = raw_article.title.lower()
        content_lower = (raw_article.summary or raw_article.content or '').lower()
        
        # –ö–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞ –¥–ª—è –ú–°–ë
        smb_keywords = [
            'automation', 'crm', 'small business', 'startup', 'entrepreneur',
            'productivity', 'efficiency', 'cost-effective', 'affordable',
            'chatbot', 'ai tool', 'business tool', 'workflow'
        ]
        
        # –†–∞—Ö—É—î–º–æ —Å–ø—ñ–≤–ø–∞–¥—ñ–Ω–Ω—è
        matches = sum(1 for keyword in smb_keywords if keyword in title_lower or keyword in content_lower)
        
        # –ë–∞–∑–æ–≤–∏–π —Å–∫–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤—ñ —Å–ø—ñ–≤–ø–∞–¥—ñ–Ω—å
        base_score = min(10, max(3, matches + 3))
        
        # –ö–∞—Ç–µ–≥–æ—Ä—ñ—è –Ω–∞ –æ—Å–Ω–æ–≤—ñ –¥–∂–µ—Ä–µ–ª–∞
        category_map = {
            'ai': 'automation',
            'automation': 'automation', 
            'crm': 'crm',
            'chatbots': 'chatbots',
            'ecommerce': 'ecommerce'
        }
        
        category = category_map.get(raw_article.source.category, 'general')
        
        return RelevanceAnalysis(
            article_id=raw_article.id,
            title=raw_article.title,
            relevance_score=base_score,
            category_match=category,
            target_audience='general',
            business_impact='medium',
            implementation_complexity='medium',
            cost_implications='medium-cost',
            key_benefits=['potential business value'],
            potential_concerns=['requires evaluation'],
            confidence_level=0.6,
            analysis_reasoning='Fallback analysis based on keyword matching'
        )

    def get_analysis_statistics(self) -> Dict:
        """–ü–æ–≤–µ—Ä—Ç–∞—î —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ä–æ–±–æ—Ç–∏ –∞–Ω–∞–ª—ñ–∑–∞—Ç–æ—Ä–∞"""
        return {
            'audience_profile': self.audience_profile,
            'scoring_weights': self.scoring_weights,
            'total_analyzed': getattr(self, '_total_analyzed', 0),
            'avg_relevance_score': getattr(self, '_avg_score', 0)
        }