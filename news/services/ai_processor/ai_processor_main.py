import time
from typing import Dict, Optional
from django.utils import timezone
from django.db.models import Sum, Q, F
from datetime import datetime, timedelta

from .ai_processor_content import AIContentProcessor
from .ai_processor_helpers import AIProcessorHelpers
from news.models import RawArticle, ProcessedArticle, AIProcessingLog
from .ai_processor_database import AIProcessorDatabase 

class AINewsProcessor(AIContentProcessor, AIProcessorHelpers, AIProcessorDatabase):
    """–ì–æ–ª–æ–≤–Ω–∏–π AI –ø—Ä–æ—Ü–µ—Å–æ—Ä –¥–ª—è –Ω–æ–≤–∏–Ω - –æ—Å–Ω–æ–≤–Ω–∞ –æ–±—Ä–æ–±–∫–∞"""

    def process_article(self, raw_article: RawArticle) -> Optional[ProcessedArticle]:
        """–û–±—Ä–æ–±–ª—è—î –æ–¥–Ω—É —Å–∏—Ä—É —Å—Ç–∞—Ç—Ç—é —á–µ—Ä–µ–∑ AI –∑ FiveFilters –∑–±–∞–≥–∞—á–µ–Ω–Ω—è–º"""
        start_time = time.time()
        self.logger.info(f"[AI] –û–±—Ä–æ–±–∫–∞ —Å—Ç–∞—Ç—Ç—ñ: {raw_article.title[:50]}...")

        try:
            # 0) –ù–û–í–ò–ô –ö–†–û–ö: –ó–±–∞–≥–∞—á–µ–Ω–Ω—è FiveFilters –ü–ï–†–ï–î AI –æ–±—Ä–æ–±–∫–æ—é
            enhanced_content = self._enhance_with_fivefilters(raw_article)
            
            # 1) –ê–Ω–∞–ª—ñ–∑ + –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü—ñ—è (—Ç–µ–ø–µ—Ä –Ω–∞ –∑–±–∞–≥–∞—á–µ–Ω–æ–º—É –∫–æ–Ω—Ç–µ–Ω—Ç—ñ)
            category_info = self._categorize_article(raw_article)
            self.logger.info(f"[AI] –ö–∞—Ç–µ–≥–æ—Ä—ñ—è –≤–∏–∑–Ω–∞—á–µ–Ω–∞: {category_info['category']}")

            # 2) –¢—Ä–∏–º–æ–≤–Ω–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç (–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –∑–±–∞–≥–∞—á–µ–Ω–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç)
            processed_content = self._create_multilingual_content(raw_article, category_info)
            self.logger.info("[AI] –¢—Ä–∏–º–æ–≤–Ω–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç–≤–æ—Ä–µ–Ω–æ ‚úÖ")

            # 3) –°—Ç–æ–∫–æ–≤—ñ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∑–∞–º—ñ—Å—Ç—å AI –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó (—è–∫ —Ç–∏ —Ö–æ—Ç—ñ–ª–∞)
            from news.services.stock_image_service import stock_image_service

            self.logger.info("[IMAGE] –ü–æ—à—É–∫ —Å—Ç–æ–∫–æ–≤–æ–≥–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è...")

            # –í–∏—Ç—è–≥—É—î–º–æ –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞ –∑ –∫–æ–Ω—Ç–µ–Ω—Ç—É
            content_keywords = self._extract_keywords_from_content(
                raw_article.content or raw_article.summary or ""
            )

            try:
                image_url = stock_image_service.get_image_for_article(
                    title=raw_article.title,
                    category=category_info.get('category_slug', 'general'),
                    keywords=content_keywords
                )
                
                processed_content["ai_image_url"] = image_url
                
                if image_url:
                    self.logger.info(f"[IMAGE] –°—Ç–æ–∫–æ–≤–µ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∑–Ω–∞–π–¥–µ–Ω–æ: {image_url}")
                else:
                    self.logger.warning("[IMAGE] –°—Ç–æ–∫–æ–≤–µ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
                    
            except Exception as img_err:
                self.logger.error(f"[IMAGE] –ü–æ–º–∏–ª–∫–∞ –ø–æ—à—É–∫—É —Å—Ç–æ–∫–æ–≤–æ–≥–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è: {img_err}")
                processed_content["ai_image_url"] = None

            # 4) –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –æ–±—Ä–æ–±–ª–µ–Ω—É —Å—Ç–∞—Ç—Ç—é (—Ä–µ—à—Ç–∞ –∫–æ–¥—É –∑–∞–ª–∏—à–∞—î—Ç—å—Å—è)
            processed_article = self._save_processed_article(raw_article, processed_content)

            # 4.1) –ó–∞–ø–∏—Å—É—î–º–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
            image_url_to_save = processed_content.get("ai_image_url")
            self.logger.info(f"[IMAGE] –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è: {image_url_to_save}")
            if image_url_to_save:
                processed_article.ai_image_url = image_url_to_save
                processed_article.save(update_fields=["ai_image_url"])
                self.logger.info(f"[IMAGE] ‚úÖ –ó–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {processed_article.ai_image_url}")
            else:
                self.logger.warning("[IMAGE] ‚ö†Ô∏è –ù–µ–º–∞—î URL –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è")

            self.logger.info("[AI] –°—Ç–∞—Ç—Ç—é –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤ –±–∞–∑—É ‚úÖ")

            # 5) –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            processing_time = time.time() - start_time
            processed_content["processing_time"] = processing_time
            self._log_ai_processing(raw_article, "full_processing", processed_content)

            # –ü–æ–∑–Ω–∞—á–∞—î–º–æ —è–∫ –æ–±—Ä–æ–±–ª–µ–Ω–∏–π
            raw_article.is_processed = True
            raw_article.save(update_fields=["is_processed"])

            self.stats["processed"] += 1
            self.stats["successful"] += 1
            self.stats["total_time"] += processing_time
            self.stats["total_cost"] += processed_content.get("cost", 0.0)

            self.logger.info(
                "[SUCCESS] –°—Ç–∞—Ç—Ç—é –æ–±—Ä–æ–±–ª–µ–Ω–æ –∑–∞ %.2fs, –≤–∞—Ä—Ç—ñ—Å—Ç—å: $%.4f",
                processing_time,
                processed_content.get("cost", 0.0),
            )

            return processed_article

        except Exception as e:
            self.stats["failed"] += 1
            error_msg = str(e)

            raw_article.processing_attempts = (raw_article.processing_attempts or 0) + 1
            raw_article.error_message = error_msg
            raw_article.save(update_fields=["processing_attempts", "error_message"])

            self._log_ai_processing(raw_article, "error", {"error": error_msg})
            self.logger.error(f"[ERROR] –ü–æ–º–∏–ª–∫–∞ –æ–±—Ä–æ–±–∫–∏ —Å—Ç–∞—Ç—Ç—ñ: {error_msg}")
            return None

    def process_batch(self, limit: int = 10, category: str = None) -> Dict:
        """–û–±—Ä–æ–±–ª—è—î –ø–∞–∫–µ—Ç –Ω–µ–æ–±—Ä–æ–±–ª–µ–Ω–∏—Ö —Å—Ç–∞—Ç–µ–π"""
        
        self.logger.info(f"üöÄ –ü–∞–∫–µ—Ç–Ω–∞ –æ–±—Ä–æ–±–∫–∞: –¥–æ {limit} —Å—Ç–∞—Ç–µ–π")
        
        # –û—Ç—Ä–∏–º—É—î–º–æ –Ω–µ–æ–±—Ä–æ–±–ª–µ–Ω—ñ —Å—Ç–∞—Ç—Ç—ñ
        queryset = RawArticle.objects.filter(
            is_processed=False,
            is_duplicate=False
        ).order_by('-published_at')
        
        if category:
            queryset = queryset.filter(source__category=category)
        
        articles = list(queryset[:limit])
        
        if not articles:
            self.logger.info("üì≠ –ù–µ–º–∞—î —Å—Ç–∞—Ç–µ–π –¥–ª—è –æ–±—Ä–æ–±–∫–∏")
            return {
                'processed': 0,
                'successful': 0,
                'failed': 0,
                'total_cost': 0
            }
        
        self.logger.info(f"üìÑ –ó–Ω–∞–π–¥–µ–Ω–æ {len(articles)} —Å—Ç–∞—Ç–µ–π –¥–ª—è –æ–±—Ä–æ–±–∫–∏")
        
        # –û–±—Ä–æ–±–ª—è—î–º–æ –∫–æ–∂–Ω—É —Å—Ç–∞—Ç—Ç—é
        results = []
        for article in articles:
            result = self.process_article(article)
            results.append(result)
            
            # –ù–µ–≤–µ–ª–∏–∫–∞ –ø–∞—É–∑–∞ –º—ñ–∂ –∑–∞–ø–∏—Ç–∞–º–∏
            time.sleep(1)
        
        # –§—ñ–Ω–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        successful = len([r for r in results if r is not None])
        
        self.logger.info(
            f"üéØ –ü–∞–∫–µ—Ç–Ω–∞ –æ–±—Ä–æ–±–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {successful}/{len(articles)} —É—Å–ø—ñ—à–Ω–∏—Ö"
        )
        
        return {
            'processed': len(articles),
            'successful': successful,
            'failed': len(articles) - successful,
            'total_cost': self.stats['total_cost'],
            'total_time': self.stats['total_time']
        }

    def process_top5_by_engagement(self, days: int = 7) -> Dict:
        """–û–±—Ä–æ–±–ª—è—î —Ç–æ–ø-5 —Å—Ç–∞—Ç–µ–π –ø–æ engagement –∑–∞ –æ—Å—Ç–∞–Ω–Ω—ñ –¥–Ω—ñ"""
        
        cutoff = timezone.now() - timedelta(days=days)

        # 1) —Ç–æ–ø –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó –∑–∞ –ø–µ—Ä–µ–≥–ª—è–¥–∞–º–∏
        top_cats = (ProcessedArticle.objects
            .filter(published_at__gte=cutoff, status='published')
            .values('category__slug')
            .annotate(total_views=Sum(F('views_count_uk') + F('views_count_en') + F('views_count_pl')))
            .order_by('-total_views')[:3])

        cat_slugs = [c['category__slug'] for c in top_cats] or ['general']

        # 2) —Å–≤—ñ–∂—ñ RawArticle –∑ —Ü–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä—ñ–π (–Ω–µ –æ–±—Ä–æ–±–ª–µ–Ω—ñ)
        queryset = (RawArticle.objects
            .filter(is_processed=False, is_duplicate=False, source__category__in=cat_slugs)
            .order_by('-published_at')[:5])

        results = []
        for article in queryset:
            result = self.process_article(article)
            results.append(result)
            time.sleep(1)

        success = len([r for r in results if r])
        return {
            'processed': len(results),
            'successful': success,
            'failed': len(results) - success
        }

    def _log_ai_processing(self, raw_article: RawArticle, log_type: str, data):
        """–õ–æ–≥—É—î AI –æ–±—Ä–æ–±–∫—É"""
        try:
            # –ë–ï–ó–ü–ï–ß–ù–ï –û–¢–†–ò–ú–ê–ù–ù–Ø –ó–ù–ê–ß–ï–ù–¨
            if isinstance(data, dict):
                ai_model = data.get('ai_model_used', self.preferred_model)
                processing_time = data.get('processing_time', 0)
                cost = data.get('cost', 0)
                error_msg = data.get('error', '')
                success = 'error' not in data
            else:
                # –Ø–∫—â–æ data —Ü–µ ProcessedContent –∞–±–æ —ñ–Ω—à–∏–π –æ–±'—î–∫—Ç
                ai_model = getattr(data, 'ai_model_used', self.preferred_model)
                processing_time = getattr(data, 'processing_time', 0)
                cost = getattr(data, 'cost', 0)
                error_msg = ''
                success = True
            
            AIProcessingLog.objects.create(
                article=raw_article,
                log_type=log_type,
                model_used=ai_model,
                processing_time=processing_time,
                cost=cost,
                success=success,
                input_data={'title': raw_article.title[:100]},
                output_data={'status': 'processed' if success else 'error'},
                error_message=error_msg
            )
        except Exception as e:
            self.logger.error(f"[ERROR] –ü–æ–º–∏–ª–∫–∞ –ª–æ–≥—É–≤–∞–Ω–Ω—è: {e}")


    def _enhance_with_fivefilters(self, raw_article: RawArticle) -> str:
        """–ó–±–∞–≥–∞—á—É—î —Å—Ç–∞—Ç—Ç—é –ø–æ–≤–Ω–∏–º —Ç–µ–∫—Å—Ç–æ–º —á–µ—Ä–µ–∑ FiveFilters"""
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ –Ω–µ –∑–±–∞–≥–∞—á–µ–Ω–æ –≤–∂–µ
        original_length = len(raw_article.content or "")
        
        # –Ø–∫—â–æ –∫–æ–Ω—Ç–µ–Ω—Ç –≤–∂–µ –¥–æ–≤–≥–∏–π - –º–æ–∂–ª–∏–≤–æ –≤–∂–µ –∑–±–∞–≥–∞—á–µ–Ω–æ
        if original_length > 1500:
            self.logger.info(f"[FIVEFILTERS] –ö–æ–Ω—Ç–µ–Ω—Ç –≤–∂–µ –¥–æ–≤–≥–∏–π ({original_length} —Å–∏–º–≤), –≤–≤–∞–∂–∞—î–º–æ –ø–æ–≤–Ω–∏–º")
            # –í—Å—Ç–∞–Ω–æ–≤–ª—é—î–º–æ —Ñ–ª–∞–≥ –ø–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É
            raw_article.has_full_content = True
            raw_article.save(update_fields=['has_full_content'])
            return raw_article.content or ""
        
        try:
            from news.services.fulltext_extractor import FullTextExtractor
            
            self.logger.info(f"[FIVEFILTERS] –ó–±–∞–≥–∞—á–µ–Ω–Ω—è: {raw_article.original_url}")
            
            extractor = FullTextExtractor()
            full_content = extractor.extract_article(raw_article.original_url)
            
            if full_content and len(full_content) > original_length:
                improvement = len(full_content) - original_length
                
                # –ó–ë–ï–†–Ü–ì–ê–Ñ–ú–û –∑–±–∞–≥–∞—á–µ–Ω–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç –≤ –ë–î
                raw_article.content = full_content
                raw_article.has_full_content = True
                raw_article.save(update_fields=['content', 'has_full_content'])
                
                self.logger.info(f"[FIVEFILTERS] ‚úÖ –ó–±–∞–≥–∞—á–µ–Ω–æ: +{improvement} —Å–∏–º–≤–æ–ª—ñ–≤ ({len(full_content)} total)")
                return full_content
            else:
                self.logger.warning(f"[FIVEFILTERS] ‚ö†Ô∏è –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–±–∞–≥–∞—Ç–∏—Ç–∏ –∞–±–æ –∫–æ–Ω—Ç–µ–Ω—Ç –∫–æ—Ä–æ—Ç—à–∏–π")
                raw_article.has_full_content = False
                raw_article.save(update_fields=['has_full_content'])
                return raw_article.content or raw_article.summary or ""
                
        except Exception as e:
            self.logger.warning(f"[FIVEFILTERS] ‚ùå –ü–æ–º–∏–ª–∫–∞: {e}")
            raw_article.has_full_content = False
            raw_article.save(update_fields=['has_full_content'])
            return raw_article.content or raw_article.summary or ""


    def _extract_keywords_from_content(self, content: str) -> list:
        """–í–∏—Ç—è–≥—É—î –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞ –∑ –∫–æ–Ω—Ç–µ–Ω—Ç—É –¥–ª—è –ø–æ—à—É–∫—É –∑–æ–±—Ä–∞–∂–µ–Ω—å"""
        import re
        
        if not content:
            return []
        
        # –°—Ç–æ–ø-—Å–ª–æ–≤–∞
        stop_words = {
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 
            'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'being',
            'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could'
        }
        
        # –í–∏—Ç—è–≥—É—î–º–æ —Å–ª–æ–≤–∞
        words = re.findall(r'\b[a-zA-Z]{4,}\b', content.lower())
        
        # –†–∞—Ö—É—î–º–æ —á–∞—Å—Ç–æ—Ç—É
        word_freq = {}
        for word in words:
            if word not in stop_words:
                word_freq[word] = word_freq.get(word, 0) + 1
        
        # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ —Ç–æ–ø-5 –Ω–∞–π—á–∞—Å—Ç—ñ—à–∏—Ö
        keywords = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
        return [word for word, freq in keywords[:5]]
