import logging
import time
from typing import Dict, List, Optional, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass
from django.utils import timezone
from django.db import transaction
from django.db.models import Q, Count

from news.models import RawArticle, ProcessedArticle, NewsCategory, DailyDigest, ROIAnalytics

from news.services.ai_processor.audience_analyzer import AudienceAnalyzer
from news.services.ai_processor.ai_processor_main import AINewsProcessor

logger = logging.getLogger(__name__)


@dataclass
class PipelineResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç —Ä–æ–±–æ—Ç–∏ Smart News Pipeline"""
    date: datetime.date
    total_articles_processed: int
    top_articles_selected: int
    articles_published: int
    digest_created: bool
    roi_calculated: bool
    processing_time: float
    errors: List[str]
    success_rate: float


class SmartNewsPipeline:
    """
    –û–ü–¢–ò–ú–Ü–ó–û–í–ê–ù–ò–ô —Ä–æ–∑—É–º–Ω–∏–π –ø–∞–π–ø–ª–∞–π–Ω –æ–±—Ä–æ–±–∫–∏ –Ω–æ–≤–∏–Ω:
    1. Audience Analyzer - —Å–µ–ª–µ–∫—Ü—ñ—è —Ç–æ–ø-5 —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏—Ö —Å—Ç–∞—Ç–µ–π
    2. Full Article Parser - –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –ø–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É –¢–Ü–õ–¨–ö–ò –¥–ª—è –¢–û–ü-5
    3. Enhanced AI Analyzer - —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è LAZYSOFT —ñ–Ω—Å–∞–π—Ç—ñ–≤ –¢–Ü–õ–¨–ö–ò –¥–ª—è –¢–û–ü-5
    4. AI News Processor - –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è —Ç—Ä–∏–º–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É –¢–Ü–õ–¨–ö–ò –¥–ª—è –¢–û–ü-5
    5. –ê–≤—Ç–æ–ø—É–±–ª—ñ–∫–∞—Ü—ñ—è –¢–û–ü-5 —Å—Ç–∞—Ç–µ–π + —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –¥–∞–π–¥–∂–µ—Å—Ç—É –∑ —Ü–∏—Ö –∂–µ –¢–û–ü-5
    6. –û–Ω–æ–≤–ª–µ–Ω–Ω—è ROI –º–µ—Ç—Ä–∏–∫
    
    –ï–ö–û–ù–û–ú–Ü–Ø: —Ç—ñ–ª—å–∫–∏ 5 AI –≤–∏–∫–ª–∏–∫—ñ–≤ –∑–∞–º—ñ—Å—Ç—å –≤—Å—ñ—Ö —Å—Ç–∞—Ç–µ–π!
    """
    
    def __init__(self):
        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î–º–æ –≤—Å—ñ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏
        
        self.audience_analyzer = AudienceAnalyzer()
        self.ai_processor = AINewsProcessor()
        
        # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è
        self.top_articles_limit = 5
        self.max_processing_time = 1800  # 30 —Ö–≤–∏–ª–∏–Ω –º–∞–∫—Å–∏–º—É–º
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'total_runs': 0,
            'successful_runs': 0,
            'avg_processing_time': 0,
            'total_articles_processed': 0
        }
        
        logger.info("üöÄ Smart News Pipeline —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–æ")

    def run_daily_pipeline(self, date: Optional[datetime.date] = None, dry_run: bool = False) -> PipelineResult:
        """
        –ó–∞–ø—É—Å–∫–∞—î –ø–æ–≤–Ω–∏–π —â–æ–¥–µ–Ω–Ω–∏–π –ø–∞–π–ø–ª–∞–π–Ω –æ–±—Ä–æ–±–∫–∏ –Ω–æ–≤–∏–Ω
        
        Args:
            date: –î–∞—Ç–∞ –¥–ª—è –æ–±—Ä–æ–±–∫–∏ (–∑–∞ –∑–∞–º–æ–≤—á–∞–Ω–Ω—è–º —Å—å–æ–≥–æ–¥–Ω—ñ)
            dry_run: –¢–µ—Å—Ç–æ–≤–∏–π —Ä–µ–∂–∏–º –±–µ–∑ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è
            
        Returns:
            PipelineResult –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Ä–æ–±–æ—Ç–∏
        """
        start_time = time.time()
        
        if not date:
            date = timezone.now().date()
        
        logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ Smart News Pipeline –∑–∞ {date}")
        
        errors = []
        articles_published = 0
        digest_created = False
        roi_calculated = False
        
        try:
            # === –ö–†–û–ö 1: –°–µ–ª–µ–∫—Ü—ñ—è —Ç–æ–ø-5 —Å—Ç–∞—Ç–µ–π ===
            logger.info("üéØ –ö—Ä–æ–∫ 1: –°–µ–ª–µ–∫—Ü—ñ—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏—Ö —Å—Ç–∞—Ç–µ–π...")
            top_articles = self.audience_analyzer.get_daily_top_articles(
                date=date, 
                limit=self.top_articles_limit
            )
 
            if not top_articles:
                logger.warning(f"‚ö†Ô∏è –ù–µ–º–∞—î —Å—Ç–∞—Ç–µ–π –¥–ª—è –æ–±—Ä–æ–±–∫–∏ –∑–∞ {date}")
                return self._create_empty_result(date, time.time() - start_time)
 
            logger.info(f"‚úÖ –í–∏–±—Ä–∞–Ω–æ {len(top_articles)} —Ç–æ–ø —Å—Ç–∞—Ç–µ–π")
 
            if not dry_run:
                ProcessedArticle.objects.filter(top_selection_date=date).update(
                    is_top_article=False,
                    article_rank=None
                )

            # === –ö–†–û–ö 2: –û–±—Ä–æ–±–∫–∞ –∫–æ–∂–Ω–æ—ó —Ç–æ–ø —Å—Ç–∞—Ç—Ç—ñ ===
            processed_articles = []
 
            for i, (raw_article, relevance_analysis) in enumerate(top_articles, 1):
                logger.info(f"üìÑ –û–±—Ä–æ–±–∫–∞ —Å—Ç–∞—Ç—Ç—ñ {i}/{len(top_articles)}: {raw_article.title[:50]}...")
 
                try:
                    processed_article = self._process_single_article(
                        raw_article, relevance_analysis, dry_run
                    )
 
                    if processed_article:
                        if not dry_run:
                            # –í—Å—Ç–∞–Ω–æ–≤–ª—é—î–º–æ –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤—ñ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—ñ
                            try:
                                score = getattr(relevance_analysis, "relevance_score", None)
                                if score is None and isinstance(relevance_analysis, dict):
                                    score = relevance_analysis.get("relevance_score")
                                if isinstance(score, (int, float)):
                                    priority = max(3, min(5, int(score // 2)))
                                else:
                                    priority = 4  # –í–∏—Å–æ–∫–∏–π –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç –¥–ª—è —Ç–æ–ø-—Å—Ç–∞—Ç–µ–π
                            except Exception:
                                priority = 4  # –í–∏—Å–æ–∫–∏–π –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç –∑–∞ –∑–∞–º–æ–≤—á–∞–Ω–Ω—è–º
                            
                            processed_article.is_top_article = True
                            processed_article.article_rank = i
                            processed_article.top_selection_date = date
                            processed_article.priority = priority
                            processed_article.save(update_fields=['is_top_article', 'article_rank', 'top_selection_date', 'priority'])
                            logger.info(f"üì¢ –°—Ç–∞—Ç—Ç—é {i} –ø–æ–∑–Ω–∞—á–µ–Ω–æ —è–∫ —Ç–æ–ø-{i} –∑ –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç–æ–º {priority}")

                        processed_articles.append(processed_article)
                        articles_published += 1
                        logger.info(f"‚úÖ –°—Ç–∞—Ç—Ç—è {i} –æ–±—Ä–æ–±–ª–µ–Ω–∞ —É—Å–ø—ñ—à–Ω–æ")
                    else:
                        errors.append(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –æ–±—Ä–æ–±–∏—Ç–∏ —Å—Ç–∞—Ç—Ç—é: {raw_article.title[:50]}")
                        
                except Exception as e:
                    error_msg = f"–ü–æ–º–∏–ª–∫–∞ –æ–±—Ä–æ–±–∫–∏ —Å—Ç–∞—Ç—Ç—ñ {i}: {str(e)}"
                    errors.append(error_msg)
                    logger.error(error_msg)
            
            # === –ö–†–û–ö 3: –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –¥–∞–π–¥–∂–µ—Å—Ç—É –∑ –¢–û–ü-5 —Å—Ç–∞—Ç–µ–π ===
            if not dry_run and processed_articles:
                logger.info("üì∞ –ö—Ä–æ–∫ 3: –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –¥–∞–π–¥–∂–µ—Å—Ç—É –∑ –¢–û–ü-5 —Å—Ç–∞—Ç–µ–π...")
                try:
                    digest_created = self._create_daily_digest_from_top_articles(date, processed_articles)
                    if digest_created:
                        logger.info("‚úÖ –î–∞–π–¥–∂–µ—Å—Ç —Å—Ç–≤–æ—Ä–µ–Ω–æ –∑ –¢–û–ü-5 —Å—Ç–∞—Ç–µ–π")
                except Exception as e:
                    errors.append(f"–ü–æ–º–∏–ª–∫–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –¥–∞–π–¥–∂–µ—Å—Ç—É: {str(e)}")
                    logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –¥–∞–π–¥–∂–µ—Å—Ç—É: {e}")
            
            # === –ö–†–û–ö 4: –û–Ω–æ–≤–ª–µ–Ω–Ω—è ROI –º–µ—Ç—Ä–∏–∫ ===
            if not dry_run and processed_articles:
                logger.info("üìä –ö—Ä–æ–∫ 4: –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ ROI –º–µ—Ç—Ä–∏–∫...")
                try:
                    roi_calculated = self._update_roi_metrics(date, len(processed_articles))
                    if roi_calculated:
                        logger.info("‚úÖ ROI –º–µ—Ç—Ä–∏–∫–∏ –æ–Ω–æ–≤–ª–µ–Ω–æ")
                except Exception as e:
                    errors.append(f"–ü–æ–º–∏–ª–∫–∞ ROI —Ä–æ–∑—Ä–∞—Ö—É–Ω–∫—É: {str(e)}")
                    logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ ROI: {e}")
            
            # === –ö–†–û–ö 5: –û–Ω–æ–≤–ª–µ–Ω–Ω—è –≤—ñ–¥–∂–µ—Ç—ñ–≤ –≥–æ–ª–æ–≤–Ω–æ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏ ===
            if not dry_run and processed_articles:
                logger.info("üè† –ö—Ä–æ–∫ 5: –û–Ω–æ–≤–ª–µ–Ω–Ω—è –≤—ñ–¥–∂–µ—Ç—ñ–≤ –≥–æ–ª–æ–≤–Ω–æ—ó...")
                try:
                    top_articles_for_widget = [a for a in processed_articles if getattr(a, 'is_top_article', False)]
                    if top_articles_for_widget:
                        logger.info("üì¢ –ù–∞ –≥–æ–ª–æ–≤–Ω—É –ø—ñ–¥–µ %d —Ç–æ–ø-—Å—Ç–∞—Ç–µ–π", min(5, len(top_articles_for_widget)))
                    self._update_homepage_widgets(top_articles_for_widget[:5])
                    logger.info("‚úÖ –í—ñ–¥–∂–µ—Ç–∏ –æ–Ω–æ–≤–ª–µ–Ω–æ")
                except Exception as e:
                    errors.append(f"–ü–æ–º–∏–ª–∫–∞ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è –≤—ñ–¥–∂–µ—Ç—ñ–≤: {str(e)}")
                    logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –≤—ñ–¥–∂–µ—Ç—ñ–≤: {e}")
            
            processing_time = time.time() - start_time
            success_rate = (articles_published / len(top_articles)) * 100 if top_articles else 0
            
            # –û–Ω–æ–≤–ª—é—î–º–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            self._update_pipeline_stats(processing_time, len(top_articles), articles_published)
            
            logger.info(
                f"üéâ Pipeline –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {processing_time:.1f}—Å: "
                f"{articles_published}/{len(top_articles)} —Å—Ç–∞—Ç–µ–π ({success_rate:.1f}%)"
            )
            
            return PipelineResult(
                date=date,
                total_articles_processed=len(top_articles),
                top_articles_selected=len(top_articles),
                articles_published=articles_published,
                digest_created=digest_created,
                roi_calculated=roi_calculated,
                processing_time=processing_time,
                errors=errors,
                success_rate=success_rate
            )
            
        except Exception as e:
            processing_time = time.time() - start_time
            error_msg = f"–ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞ pipeline: {str(e)}"
            logger.error(error_msg)
            
            return PipelineResult(
                date=date,
                total_articles_processed=0,
                top_articles_selected=0,
                articles_published=0,
                digest_created=False,
                roi_calculated=False,
                processing_time=processing_time,
                errors=[error_msg],
                success_rate=0.0
            )

    def _process_single_article(self, raw_article: RawArticle, relevance_analysis, dry_run: bool = False) -> Optional[ProcessedArticle]:
        """–û–±—Ä–æ–±–ª—è—î –æ–¥–Ω—É —Å—Ç–∞—Ç—Ç—é —á–µ—Ä–µ–∑ –ø–æ–≤–Ω–∏–π –ø–∞–π–ø–ª–∞–π–Ω (FiveFilters ‚Üí insights ‚Üí AI ‚Üí publish)."""
        try:
            # 1) –ó–±–∞–≥–∞—á—É—î–º–æ –ø–æ–≤–Ω–∏–º –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º —á–µ—Ä–µ–∑ FiveFilters (—Ç—ñ–ª—å–∫–∏ –¥–ª—è —Ç–æ–ø-—Å—Ç–∞—Ç–µ–π)
            logger.info("üîç –ó–±–∞–≥–∞—á–µ–Ω–Ω—è –ø–æ–≤–Ω–∏–º –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º —á–µ—Ä–µ–∑ FiveFilters...")
            full_content = self.ai_processor._enhance_with_fivefilters(raw_article)
            if full_content and len(full_content) > 1000:
                logger.info(f"‚úÖ Full-text –æ—Ç—Ä–∏–º–∞–Ω–æ: {len(full_content)} —Å–∏–º–≤–æ–ª—ñ–≤")
                # –ü–æ–∑–Ω–∞—á–∞—î–º–æ, —â–æ —Å—Ç–∞—Ç—Ç—è –º–∞—î –ø–æ–≤–Ω–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç
                raw_article.has_full_content = True
                raw_article.save(update_fields=['has_full_content'])
            else:
                logger.warning("‚ö†Ô∏è Full-text –∫–æ—Ä–æ—Ç–∫–∏–π –∞–±–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π; –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ RSS –∫–æ–Ω—Ç–µ–Ω—Ç")
                full_content = (raw_article.content or raw_article.summary or "").strip()
                raw_article.has_full_content = False
                raw_article.save(update_fields=['has_full_content'])

            # 2) –û—Å–Ω–æ–≤–Ω–∞ AI-–æ–±—Ä–æ–±–∫–∞ –∑ –ø–æ–≤–Ω–∏–º –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º (–≥–µ–Ω–µ—Ä–∞—Ü—ñ—è –≤—Å—ñ—Ö –ø–æ–ª—ñ–≤ –≤–∫–ª—é—á–∞—é—á–∏ —ñ–Ω—Å–∞–π—Ç–∏)
            logger.info("üé® AI –æ–±—Ä–æ–±–∫–∞ —Ç–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è –∫–æ–Ω—Ç–µ–Ω—Ç—É –∑ –ø–æ–≤–Ω–∏–º —Ç–µ–∫—Å—Ç–æ–º...")
            logger.info(f"[PIPELINE] –í–∏–∫–ª–∏–∫–∞—é ai_processor.process_article –¥–ª—è '{raw_article.title[:60]}...'")
            logger.info(f"[PIPELINE] full_content –¥–æ–≤–∂–∏–Ω–∞: {len(full_content) if full_content else 0}")

            try:
                processed_article = self.ai_processor.process_article(raw_article, full_content=full_content)
                logger.info(f"[PIPELINE] process_article –ø–æ–≤–µ—Ä–Ω—É–≤: {processed_article}")
            except Exception as proc_err:
                logger.exception(f"[PIPELINE] ‚ùå EXCEPTION –≤ process_article: {proc_err}")
                processed_article = None

            if not processed_article:
                logger.error("‚ùå AI –ø—Ä–æ—Ü–µ—Å–æ—Ä –Ω–µ –∑–º—ñ–≥ –æ–±—Ä–æ–±–∏—Ç–∏ —Å—Ç–∞—Ç—Ç—é")
                return None

            logger.info(f"[PIPELINE] ‚úÖ –°—Ç–∞—Ç—Ç—è –æ–±—Ä–æ–±–ª–µ–Ω–∞ —É—Å–ø—ñ—à–Ω–æ: {processed_article.id}")

            # 3) –ü–æ–≤–Ω–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç —Ç–µ–ø–µ—Ä –≥–µ–Ω–µ—Ä—É—î—Ç—å—Å—è –≤ ai_processor_main.py

            # 4) –ü—É–±–ª—ñ–∫—É—î–º–æ —Å—Ç–∞—Ç—Ç—é (–≤—Å—Ç–∞–Ω–æ–≤–ª—é—î–º–æ —Å—Ç–∞—Ç—É—Å, –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç, –¥–∞—Ç—É)
            if not dry_run:
                base_priority = 3
                try:
                    score = getattr(relevance_analysis, "relevance_score", None)
                    if score is None and isinstance(relevance_analysis, dict):
                        score = relevance_analysis.get("relevance_score")
                    if isinstance(score, (int, float)):
                        base_priority = max(3, min(5, int(score // 2)))
                except Exception:
                    pass

                processed_article.status = "published"
                processed_article.priority = base_priority
                processed_article.published_at = timezone.now()
                processed_article.save()

                logger.info(f"üì¢ –°—Ç–∞—Ç—Ç—é –æ–ø—É–±–ª—ñ–∫–æ–≤–∞–Ω–æ –∑ –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç–æ–º {processed_article.priority}")

            return processed_article

        except Exception as e:
            logger.exception(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ–±—Ä–æ–±–∫–∏ —Å—Ç–∞—Ç—Ç—ñ: {e}")
            return None



    def _create_daily_digest_from_top_articles(self, date: datetime.date, top_articles: List[ProcessedArticle]) -> bool:
        """–°—Ç–≤–æ—Ä—é—î —â–æ–¥–µ–Ω–Ω–∏–π –¥–∞–π–¥–∂–µ—Å—Ç –¢–Ü–õ–¨–ö–ò –∑ –¢–û–ü-5 —Å—Ç–∞—Ç–µ–π."""
        try:
            if not top_articles:
                logger.info("üì≠ –ù–µ–º–∞—î –¢–û–ü —Å—Ç–∞—Ç–µ–π –¥–ª—è –¥–∞–π–¥–∂–µ—Å—Ç—É")
                return False

            # –°—Ç–≤–æ—Ä—é—î–º–æ –¥–∞–π–¥–∂–µ—Å—Ç –∑ –¢–û–ü-5 —Å—Ç–∞—Ç–µ–π
            digest, created = DailyDigest.objects.get_or_create(
                date=date,
                defaults={
                    'title_en': f"LAZYSOFT Top Tech News - {date.strftime('%B %d, %Y')}",
                    'title_pl': f"LAZYSOFT Top wiadomo≈õci technologiczne - {date.strftime('%d %B %Y')}",
                    'title_uk': f"LAZYSOFT –¢–æ–ø —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ—á–Ω–∏—Ö –Ω–æ–≤–∏–Ω - {date.strftime('%d %B %Y')}",
                    'intro_text_en': f"Our top {len(top_articles)} curated tech insights for {date}",
                    'intro_text_pl': f"Nasze top {len(top_articles)} wyselekcjonowanych wiadomo≈õci technologicznych za {date}",
                    'intro_text_uk': f"–ù–∞—à—ñ —Ç–æ–ø-{len(top_articles)} –≤—ñ–¥—ñ–±—Ä–∞–Ω–∏—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ—á–Ω–∏—Ö —ñ–Ω—Å–∞–π—Ç—ñ–≤ –∑–∞ {date}",
                    'total_articles': len(top_articles),
                    'is_generated': True,
                    'is_published': True,
                    'published_at': timezone.now()
                }
            )

            # –û–Ω–æ–≤–ª—é—î–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Å—Ç–∞—Ç–µ–π —è–∫—â–æ –¥–∞–π–¥–∂–µ—Å—Ç –≤–∂–µ —ñ—Å–Ω—É–≤–∞–≤
            if not created:
                digest.total_articles = len(top_articles)
                digest.save()

            logger.info(f"‚úÖ –î–∞–π–¥–∂–µ—Å—Ç {'—Å—Ç–≤–æ—Ä–µ–Ω–æ' if created else '–æ–Ω–æ–≤–ª–µ–Ω–æ'} –∑ {len(top_articles)} –¢–û–ü —Å—Ç–∞—Ç–µ–π")
            return True

        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –¥–∞–π–¥–∂–µ—Å—Ç—É: {e}")
            return False

    def _update_roi_metrics(self, date: datetime.date, articles_processed: int) -> bool:
        """–û–Ω–æ–≤–ª—é—î ROI –º–µ—Ç—Ä–∏–∫–∏"""
        
        try:
            roi = ROIAnalytics.calculate_daily_metrics(date)
            logger.info(f"‚úÖ ROI —Ä–æ–∑—Ä–∞—Ö–æ–≤–∞–Ω–æ: ${roi.net_savings:.2f} –µ–∫–æ–Ω–æ–º—ñ—ó")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ ROI —Ä–æ–∑—Ä–∞—Ö—É–Ω–∫—É: {e}")
            return False

    def _update_homepage_widgets(self, processed_articles: List[ProcessedArticle]):
        """–û–Ω–æ–≤–ª—é—î –≤—ñ–¥–∂–µ—Ç–∏ –Ω–æ–≤–∏–Ω –Ω–∞ –≥–æ–ª–æ–≤–Ω—ñ–π —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ"""
 
        try:
            logger.info("üè† –û–Ω–æ–≤–ª–µ–Ω–Ω—è –≤—ñ–¥–∂–µ—Ç—ñ–≤ ‚Äî –æ—Ç—Ä–∏–º–∞–Ω–æ %d —Å—Ç–∞—Ç–µ–π", len(processed_articles))
            # –¢—É—Ç –º–æ–∂–Ω–∞ –¥–æ–¥–∞—Ç–∏ –ª–æ–≥—ñ–∫—É –æ–Ω–æ–≤–ª–µ–Ω–Ω—è –∫–µ—à—É –≤—ñ–¥–∂–µ—Ç—ñ–≤
            # –ù–∞–ø—Ä–∏–∫–ª–∞–¥, —ñ–Ω–≤–∞–ª—ñ–¥—É–≤–∞—Ç–∏ –∫–µ—à –∞–±–æ –æ–Ω–æ–≤–∏—Ç–∏ —Å—Ç–∞—Ç–∏—á–Ω—ñ —Ñ–∞–π–ª–∏
 
            from django.core.cache import cache
 
            # –û—á–∏—â–∞—î–º–æ –∫–µ—à –≤—ñ–¥–∂–µ—Ç—ñ–≤ –Ω–æ–≤–∏–Ω
            cache_keys = [
                'homepage_news_uk',
                'homepage_news_en', 
                'homepage_news_pl',
                'featured_articles',
                'news_digest'
            ]
 
            for key in cache_keys:
                cache.delete(key)
 
            logger.info("‚úÖ –ö–µ—à –≤—ñ–¥–∂–µ—Ç—ñ–≤ –æ—á–∏—â–µ–Ω–æ")
 
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è –≤—ñ–¥–∂–µ—Ç—ñ–≤: {e}")

    def _update_pipeline_stats(self, processing_time: float, total_articles: int, successful_articles: int):
        """–û–Ω–æ–≤–ª—é—î —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ä–æ–±–æ—Ç–∏ –ø–∞–π–ø–ª–∞–π–Ω–∞"""
        
        self.stats['total_runs'] += 1
        if successful_articles > 0:
            self.stats['successful_runs'] += 1
        
        # –û–Ω–æ–≤–ª—é—î–º–æ —Å–µ—Ä–µ–¥–Ω—ñ–π —á–∞—Å
        self.stats['avg_processing_time'] = (
            (self.stats['avg_processing_time'] * (self.stats['total_runs'] - 1) + processing_time) /
            self.stats['total_runs']
        )
        
        self.stats['total_articles_processed'] += total_articles

    def _create_empty_result(self, date: datetime.date, processing_time: float) -> PipelineResult:
        """–°—Ç–≤–æ—Ä—é—î –ø–æ—Ä–æ–∂–Ω—ñ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç —è–∫—â–æ –Ω–µ–º–∞—î —Å—Ç–∞—Ç–µ–π"""
        
        return PipelineResult(
            date=date,
            total_articles_processed=0,
            top_articles_selected=0,
            articles_published=0,
            digest_created=False,
            roi_calculated=False,
            processing_time=processing_time,
            errors=["–ù–µ–º–∞—î —Å—Ç–∞—Ç–µ–π –¥–ª—è –æ–±—Ä–æ–±–∫–∏"],
            success_rate=0.0
        )

    def get_pipeline_statistics(self) -> Dict:
        """–ü–æ–≤–µ—Ä—Ç–∞—î —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ä–æ–±–æ—Ç–∏ –ø–∞–π–ø–ª–∞–π–Ω–∞"""
        
        return {
            **self.stats,
            'components': {
                'full_parser': self.full_parser.get_parsing_stats(),
                'audience_analyzer': self.audience_analyzer.get_analysis_statistics(),
                'ai_processor': self.ai_processor.get_processing_stats()
            }
        }

    def health_check(self) -> Dict:
        """–ü–µ—Ä–µ–≤—ñ—Ä—è—î –∑–¥–æ—Ä–æ–≤'—è –≤—Å—ñ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤ –ø–∞–π–ø–ª–∞–π–Ω–∞"""
        
        health = {}
        
        try:
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ AI –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è
            test_article = RawArticle.objects.first()
            if test_article:
                # –®–≤–∏–¥–∫–∏–π —Ç–µ—Å—Ç Audience Analyzer
                analysis = self.audience_analyzer.analyze_article_relevance(test_article)
                health['audience_analyzer'] = analysis.relevance_score > 0
                
                # –¢–µ—Å—Ç AI –ø—Ä–æ—Ü–µ—Å–æ—Ä–∞ (—Ç—ñ–ª—å–∫–∏ OpenAI)
                health['ai_processor'] = self.ai_processor.openai_client is not None
            else:
                health['audience_analyzer'] = False
                health['ai_processor'] = False
            
            # –¢–µ—Å—Ç Full Parser
            health['full_parser'] = True  # –ó–∞–≤–∂–¥–∏ –¥–æ—Å—Ç—É–ø–Ω–∏–π
            
            health['overall'] = all(health.values())
            
        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ health check: {e}")
            health = {'overall': False, 'error': str(e)}
        
        return health